\section{The Mumford-Shah Functional} % (fold)
\label{sec:the_mumford_shah_functional}
    
    We saw that solving the ROF Model and TVL1 Model, respectively, depended mainly on finding the right choices of the convex conjugates and the proximity operator. Now, we introduce a highly non-convex functional which is more difficult to solve. The problem of the proposed method in this section is, that there is - to date - no proof that it convergences to a global minimum. Even though, one can see that it yields to high quality solutions. But, of course one wants to be sure to compute the global optimum. Therefore, we discuss in the next chapter a method to minimize the Mumford-Shah functional optimally. For now, let us first give the definition of this functional.

    \begin{definition}[The Mumford-Shah Functional \cite{Strekalovskiy-Cremers-eccv14}] % (fold)
    \label{def:the_mumford_shah_functional}

        Let $\Omega \subset \mathbb{R}^{2}$ be a rectangular image domain. In order to approximate an input image $g: \Omega \longrightarrow \mathbb{R}$ in terms of a piecewise smooth function $u: \Omega \longrightarrow \mathbb{R}$, the Mumford-Shah Functional is given by
                
                \begin{equation}
                    E(u) = \int_{\Omega} (u - g)^{2} dx + R(u) = \int_{\Omega} (u - g)^{2} dx + \lambda \int_{\Omega \setminus K} |\nabla u|^{2} dx + \nu |K|,
                \end{equation}
                \label{eq:the_mumford_shah_functional}
            
            where $\nu, \lambda > 0$ are weighting parameters, $K = K_{1} \cup ... \cup K_{N}$ and $|K|$ denotes the length of the curves in $K$.

    \end{definition}
    % definition the_mumford_shah_functional (end)

    \begin{remark}
        In this section we follow the represenation of the Mumford-Shah Functional of \cite{Strekalovskiy-Cremers-eccv14}. This is equivalent to Equation \ref{eq:the_mumford_shah_functional}, but has some advantages for our notation. In Chapter \ref{cha:a_first_order_primal_dual_algorithm_for_minimizing_the_mumford_shah_functional} we will state a second notation.
    \end{remark}

    This functional differs from the ones in the previous sections. The first term, the data fidelity term, remains the same as in the ROF or TVL1 Model. The approximation $u$ should be as close to the input image $g$ as possible. The regularizer consists of two terms. The first term of this kind of regularizer, also known as Mumford-Shah regularizer, uses again the gradient, but not in the set $\Omega$ itself. Instead it states that the approximation $u$ is not allowed to change too much in sets $\Omega \setminus K_{i}$. We call $K_{i}$ the discontinouty sets (or jump sets) for all $i = 1, ..., N$ and curves $u_{i}$. The gradient is only taken into account in these regions if they are smooth enough. These discontinouties of the sets $K_{i}$ are also measured and taken into account in the energy $E(u)$. This means, that all curves $K_{i}$ should be regular in the sense of measure theory. Here, we also find two weighting parameters $\nu$ and $\lambda$. Where $\lambda$ handles the tradeoff between the first two terms as in the ROF or TVL1 Model, $\nu$ controls the length of the discontinouty sets. A smaller $\nu$ yields to a smoother image, where a higher $\nu$ leads to sharper edges in our images. The parameter $\lambda$ yet plays another important role. If one chooses a $\lambda$ small enough, then our model is also called piecewise-smooth Mumford-Shah Model. On the other hand, in the limiting case $\lambda \longrightarrow \infty$, we can only attain a minimum if we set $\nabla u = 0$ in $\Omega \setminus K$. Then the model is known as the piecewise-constant Mumford-Shah Model.
    In \cite{Strekalovskiy-Cremers-eccv14} Strekalovskiy and Cremers proposed to rewrite this functional in a discrete setting by first defining the discrete regularizer function by
        \begin{equation}
            R_{MS}(u) = \min(\lambda||u||_{2}^{2},\nu).
        \label{eq:ms_regularizer}
        \end{equation}
    Then the discrete Mumford-Shah Model can be expressed by
        \begin{equation}
            \min_{u \in X} E_{MS}(u) = \min_{u \in X} ||u - g||_{2}^{2} + R_{MS}(\nabla u).
        \label{eq:discrete_mumford_shah_functional}
        \end{equation}
    According to \cite{Strekalovskiy-Cremers-eccv14} the idea behind this formulation is to model the discontinouity set $K$ explicitly in terms of the function $u$. This means, that $K$ is the set of all points where the minimum in \ref{eq:ms_regularizer} attains $\nu$. In other words, if the gradient $\nabla u$ is large enough we have for the explicit set $K_{MS}$:
        \begin{equation}
            K_{MS} = \bigg\{ (i, j) \in \Omega : ||\nabla u_{i, j}||_{2}^{2} \ge \sqrt{\frac{\nu}{\lambda}} \bigg\}.
        \label{eq:set_k_ms}
        \end{equation}
    We can check easy that for a point $(i, j) \in K_{MS}$ we observe
        $$
            R_{MS}(\nabla u_{i, j}) = \min(\underbrace{\lambda||\nabla u_{i, j}||_{2}^{2}}_{\ge \lambda \sqrt{\frac{\nu}{\lambda}}^{2} = \nu}, \nu) = \nu
        $$
    and if $(i, j) \notin K_{MS}$ we have
        $$
            R_{MS}(\nabla u_{i, j}) = \min(\underbrace{\lambda||\nabla u_{i, j}||_{2}^{2}}_{< \lambda \sqrt{\frac{\nu}{\lambda}}^{2} = \nu}, \nu) = \lambda||\nabla u_{i, j}||_{2}^{2}.
        $$

    \begin{remark}
        In the piecewise-constant case, where $\lambda \longrightarrow \infty$, Equation \ref{eq:ms_regularizer} changes to
            \begin{equation}
                R_{MS}(u) = 
                    \begin{dcases*}
                        \nu & \textnormal{if $u \ne 0$,} \\
                        0 & \textnormal{else}.
                    \end{dcases*}
            \label{eq:ms_regularizer_piecewise_constant}
            \end{equation}
    \end{remark}

    \subsection{Mumford-Shah as Saddle-Point Problem} % (fold)
    \label{sub:mumford_shah_as_saddle_point_problem}

        Again, we try to formulate the Mumford-Shah Model as a saddle-point problem to be able to apply the second primal-dual algorithm of Section \ref{sec:a_firs_order_primal_dual_algorithm}. In the sense of our notations from the previous section we have
            \begin{equation}
                \min_{u \in X}\,\, F(\nabla u) + G(u) = R_{MS}(\nabla u) + ||u - g||_{2}^{2}
                \label{eq:primal_mumford_shah_model}
            \end{equation}
        This is the primal formulation for the discrete Mumford-Shah Model. Applying now the Legendre-Fenchel conjugate on the Mumford-Shah regularizer $R_{MS}$ we get the primal-dual formulation with
            \begin{equation}
                \min_{u \in X}\, \max_{p \in Y}\,\, \langle p, \nabla \, u \rangle_{X} - R_{MS}^{\ast}(p) + G(u).% = \min_{u \in X}\, \max_{p \in Y}\,\, -\langle \nabla^{T}\,p, u \rangle_{X} - R_{MS}^{\ast}(p) + G(u).
            \label{eq:primal_dual_mumford_shah_model}
            \end{equation}
        As before, we want to compute the convex conjugate of $R_{MS}$. We have
            $$
                R_{MS}^{\ast}(p) = \sup_{u \in X} \langle p, u \rangle - R_{MS}(u) = \sup_{p \in Y} \langle p, u \rangle - \min(\lambda ||u||_{2}^{2}, \nu).
            $$
        We need to distinguish both cases of our minimum function.
            \begin{enumerate}
                \item Assume that $\min(\lambda ||u||_{2}^{2}, \nu) = \nu$. We get
                    $$
                        \sup_{u \in X} \langle p, u \rangle - \nu = S_{Y}(p) - \nu.
                    $$
                where $S_{Y}(p)$ denotes the support function of $Y$ for a point $p \in Y$. Clearly, if $p \neq 0$, then the supremum over all $u$ is $\infty$. But if $p = 0$, this expression becomes
                    $$
                        S_{Y}(0) - \nu = - \nu \Longleftrightarrow -S_{Y}(0) + \nu = \nu.
                    $$
                \item On the other side, if we assume that $\min(\lambda ||u||_{2}^{2}, \nu) = \lambda ||u||_{2}^{2}$ we can apply Example \ref{ex:legendre_fenchel_conjugate_example} 2. and 4. Let $\hat{R}(u) = \frac{||u||_{2}^{2}}{2}$, then the Legendre-Fenchel conjugate of $R_{MS}(u) = 2\lambda \hat{R}(u)$ is $2\lambda \hat{R}^{\ast}(\frac{p}{2\lambda})$. We get
                    \begin{equation}
                        R_{MS}^{\ast}(p) = 2\lambda \hat{R}^{\ast}(\frac{p}{2\lambda}) = 2\lambda \frac{||\frac{p}{2\lambda}||_{2}^{2}}{2} = \frac{||p||_{2}^{2}}{4\lambda}.
                    \label{eq:mumford_shah_convex_conjugate}
                    \end{equation}
                If we now make use of Proposition \ref{prop:convex_subgradient}, we have that $p \in \partial R_{MS}(u)$ if and only if
                    $$
                        \langle p, u \rangle - R_{MS}(u) = R_{MS}^{\ast}(p).
                    $$
                First, let us verify that this expression holds.
                    \begin{proof}
                        "$\Longrightarrow$":\\
                        Let $p \in \partial R_{MS}(u) = 2\lambda u$, then this is equivalent to $p = 2\lambda u \Longleftrightarrow u = \frac{p}{2\lambda}$. Pluging this into the equation we get
                            \begin{eqnarray}
                                \langle p, u \rangle - R_{MS}(u) &=& \langle p, \frac{p}{2\lambda} \rangle - \lambda||\frac{p}{2\lambda}||_{2}^{2} = \frac{1}{2\lambda} ||p||_{2}^{2} - \frac{\lambda}{4\lambda^{2}} ||p||_{2}^{2} \notag \\
                                &=& \frac{1}{\lambda} ||p||_{2}^{2} - \frac{1}{4\lambda} ||p||_{2}^{2} \notag \\
                                &=& \frac{1}{4\lambda} ||p||_{2}^{2} = R_{MS}^{\ast}(p).
                            \end{eqnarray}
                        "$\Longleftarrow$":\\
                        Now, rewrite the equation to
                            $$
                                R_{MS}(u) = \langle p, u \rangle - R_{MS}^{\ast}(p).
                            $$
                        Taking the subdifferential on $R_{MS}$ we observe
                            $$
                                \partial R_{MS}(u) = \partial \bigg( \langle p, u \rangle - R_{MS}^{\ast}(p) \bigg) \Longleftrightarrow \lambda u = p \Longleftrightarrow p \in \lambda u = \partial R_{MS}(u).
                            $$
                        \qed
                    \end{proof}
                Knowing that $p \in \partial R_{MS}(u)$ or $u = \frac{p}{2\lambda}$ respectively, we have that Equation \ref{eq:mumford_shah_convex_conjugate} only holds if $\lambda ||u||_{2}^{2} \le \nu$ or if $\lambda ||\frac{p}{2\lambda}||_{2}^{2} = \frac{||p||_{2}^{2}}{4\lambda} \le \nu$ or equivalently $||p||_{2} \le \sqrt{2\lambda \nu}$.
            \end{enumerate}
        Overall, the convex conjugate of the Mumford-Shah regularizer is given by
            \begin{equation}
                R_{MS}^{\ast}(p) =
                    \begin{dcases*}
                        \frac{||p||_{2}^{2}}{4\lambda}, & \textnormal{if $||p||_{2} \in (0, \sqrt{2\lambda \nu}]$,} \\
                        \nu, & \textnormal{if $||p||_{2} = 0$,} \\
                        \infty, & \textnormal{else.}
                    \end{dcases*}
                \label{eq:r_star}
            \end{equation}
        % To simplify this equation, we define the set $A$ with
        %     \begin{equation}
        %         A := \big\{ p \in Y : ||p||_{2} \le \sqrt{\lambda \nu} \big\}.
        %     \label{eq:set_a}
        %     \end{equation}
        % Then we can express the 
        To derive the dual formulation of the Mumford-Shah Model we would just need to plug the derived definition of the convex conjugate into the dual problem.
            $$
                \max_{p \in Y} - (G^{\ast}(-K^{\ast}p) + R_{MS}^{\ast}(p)) = \max_{p \in Y} -(||\nabla^{T}p - g||_{2}^{2}) + \frac{||p||_{2}^{2}}{2\lambda},
            $$
        as long as $||p||_{2} \in (0, \sqrt{\lambda \nu}]$. If $||p||_{2} = 0$ we have
            $$
                \max_{p \in Y} - (G^{\ast}(-K^{\ast}p) + R_{MS}^{\ast}(p)) = \max_{0 = p \in Y} -(||\nabla^{T}p - g||_{2}^{2}) + \nu = -(g^{T}g + \nu) = -g^{T}g - \nu.
            $$
        In all other cases we would observe
            $$
                \max_{p \in Y} - (G^{\ast}(-K^{\ast}p) + R_{MS}^{\ast}(p)) = \max_{p \in Y} -(||\nabla^{T}p - g||_{2}^{2} + \infty) = -\infty.
            $$
        We are now interested in the proximity operators for our model, which we compute in the next subsection.

    % subsection mumford_shah_as_saddle_point_problem (end)

    \subsection{The Proximity Operators of the Mumford-Shah Model} % (fold)
    \label{sub:the_proximity_operators_of_the_mumford_shah_model}
        
        Let us now compute the proximity operators of our proposed model. The operator for the function $G$ can be adapted from Equation \ref{eq:proximity_operator_g_rof}, because our data fidelity term is the same as in the ROF Model. Then we have pointwise for all $i = 1, ..., N$ and $j = 1, ..., M$

            \begin{equation}
                (\textnormal{Id} + \tau\,\partial\,G)^{-1}(\tilde{u}) = u \Longleftrightarrow u_{i,j} = \frac{\tilde{u}_{i,j} + \tau\nu g}{1 + \tau\sigma}.
            \label{eq:proximity_operator_g_mumford_shah}
            \end{equation}

        The more interesting case is to compute the proximity operator for $R_{MS}^{\ast}(p)$. Here, we have two possibilities to derive a formula for this. In \cite{Strekalovskiy-Cremers-eccv14} one can find this operator derived by using Moreau's Identity. We will show this case, too. But first we want to provide another way to derive the formula. We just computed $R_{MS}^{\ast}(p)$, so we easily apply the definition of the proximity operator. Since the third case of $R_{MS}^{\ast}$ where the conjugate is $\infty$ is useless for us, we skip this case and first consider the case where $||p||_{2} = 0$ which is equivalent to $p = 0$. For that we already see that if $||p||_{2} = 0$ we set $p = 0$. If $||p||_{2} \in (0, \sqrt{\lambda \nu}]$, then we get
            \begin{eqnarray}
                \textnormal{prox}_{R_{MS}^{\ast}(p)}^{\sigma}(\tilde{p}) &=& \min_{p} \frac{||p - \tilde{p}||_{2}^{2}}{2} + \sigma R_{MS}^{\ast}(p) \notag \\
                &=& \min_{p} \frac{||p - \tilde{p}||_{2}^{2}}{2} + \sigma \frac{||p||_{2}^{2}}{4\lambda} \notag
            \end{eqnarray}
        Again, we define $\mathcal{L}(p) = \frac{||p - \tilde{p}||_{2}^{2}}{2} + \sigma \frac{||p||_{2}^{2}}{4\lambda}$ then $\min\limits_{p} \mathcal{L}(p)$ is equivalent to solve $\nabla \mathcal{L}(p) = 0$. We have
            $$
                \nabla \mathcal{L}(p) = p - \tilde{p} + \sigma \frac{p}{2\lambda} = 0 \Longleftrightarrow (1 + \frac{\sigma}{2\lambda}) p = \tilde{p}.
            $$
        Solving for $p$ leads us to
            $$
                p = \frac{\tilde{p}}{1 + \frac{\sigma}{2\lambda}} = \frac{2\lambda}{2\lambda + \sigma} \tilde{p}.
            $$
        Now, that we know that $p = \frac{2\lambda}{2\lambda + \sigma} \tilde{p}$ we can plug this into the constraint to see for which condition on $\tilde{g}$ the computed value for $p$ holds. Then with $\frac{||p||_{2}^{2}}{4\lambda} \le \nu$ and $p \ne 0$ we have
            \begin{eqnarray}
                \frac{||\frac{2\lambda}{2\lambda + \sigma} \tilde{p}||_{2}^{2}}{4\lambda} \le \nu &\Longleftrightarrow& \frac{4\lambda^{2}}{4\lambda(4\lambda + \sigma)^{2}} ||\tilde{p}||_{2}^{2} \le \nu \notag \\
                &\Longleftrightarrow& \frac{4\lambda}{(4\lambda + \sigma)^{2}} ||\tilde{p}||_{2}^{2} \le \nu \notag \\
                &\Longleftrightarrow& ||\tilde{p}||_{2}^{2} \le \frac{\nu}{4\lambda} (4\lambda + \sigma)^{2} \notag \\
                &\Longleftrightarrow& ||\tilde{p}||_{2} \le \sqrt{\frac{\nu}{4\lambda}} \, (4\lambda + \sigma) \notag
            \end{eqnarray}
        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        Then the proximity operator is defined by
            \begin{equation}
                p = (\textnormal{Id} + \sigma \partial R_{MS}^{\ast})^{-1}(\tilde{p}) \Longleftrightarrow p_{i,j} =
                    \begin{dcases*}
                        \frac{\lambda}{\lambda + \sigma} \tilde{p}, & \textnormal{if $||\tilde{p}||_{2} \le \sqrt{\frac{\nu}{\lambda}} \, (\lambda + \sigma)$,} \\
                        0 & \textnormal{else.}
                    \end{dcases*}
                \label{eq:proximity_operator_f_star}
            \end{equation}
        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

        As mentioned before we have a second possibility to derive the proximity operator for $R_{MS}^{\ast}$. For this we use Moreau's Theorem, c.f. Theorem \ref{def:moreau_identity}. It states, that we can compute $(\textnormal{Id} + \sigma \partial R_{MS}^{\ast})^{-1}(\tilde{p})$ by
            $$
                (\textnormal{Id} + \sigma \partial R_{MS}^{\ast})^{-1}(\tilde{p}) = \tilde{p} - \sigma (\textnormal{Id} + \frac{1}{\sigma} \partial R_{MS})^{-1}(\tilde{u}).
            $$
        Then we can compute the proximity operator of $R_{MS}$ with
            $$
                (\textnormal{Id} + \tau \partial R_{MS})^{-1}(\tilde{u}) = \min_{u} \frac{||u - \tilde{u}||_{2}^{2}}{2} + \tau \min(\lambda ||u||_{2}^{2}, \nu).
            $$
        First, we consider the case where the minimum function attains $\nu$. Then
            \begin{eqnarray}
                \min_{u} \frac{||u - \tilde{u}||_{2}^{2}}{2} + \nu\tau &\Longleftrightarrow& \nabla \bigg( \frac{||u - \tilde{u}||_{2}^{2}}{2} + \nu\tau \bigg) = 0 \notag \\
                &\Longleftrightarrow& u - \tilde{u} = 0 \notag \\
                &\Longrightarrow& u = \tilde{u}. \notag
            \end{eqnarray}
        Now, assume that $\lambda ||u||_{2}^{2} < \nu$:
            \begin{eqnarray}
                \min_{u} \frac{||u - \tilde{u}||_{2}^{2}}{2} + \lambda\tau ||u||_{2}^{2} &\Longleftrightarrow& \nabla \bigg( \frac{||u - \tilde{u}||_{2}^{2}}{2} + \tau \lambda ||u||_{2}^{2} \bigg) = 0 \notag \\
                &\Longleftrightarrow& (u - \tilde{u} + 2\lambda\tau u) = 0 \notag \\
                &\Longleftrightarrow& (1 + 2\lambda\tau) u = \tilde{u} \notag \\
                &\Longleftarrow& u = \frac{\tilde{u}}{(1 + 2\lambda\tau)} \notag
            \end{eqnarray}
        Again, since we know that at least $\lambda \frac{||u||_{2}^{2}}{2} \le \nu$ holds we can compute the constraint by plugging $u = \frac{\tilde{u}}{(1 + 2\lambda\tau)}$ into the inequality. We observe
            \begin{eqnarray}
                \lambda ||u||_{2}^{2} \le \nu &\Longleftrightarrow& \lambda ||\frac{\tilde{u}}{(1 + 2\lambda\tau)}||_{2}^{2} \le \nu \notag \\
                &\Longleftrightarrow& \frac{\lambda}{(1 + 2\lambda\tau)^{2}} ||\tilde{u}||_{2}^{2} \le \nu \notag \\
                &\Longleftrightarrow& ||\tilde{u}||_{2}^{2} \le \frac{\nu}{\lambda}(1 + 2\lambda\tau)^{2} \notag \\
                &\Longrightarrow& ||\tilde{u}||_{2} \le \sqrt{\frac{\nu}{\lambda}}(1 + 2\lambda\tau) \notag
            \end{eqnarray}
        The proximity operator for $R_{MS}$ for all $i = 1, ..., N$ and $j = 1, ..., M$ is then given pointwise by
            \begin{equation}
                u = (\textnormal{Id} + \tau \partial R_{MS})^{-1}(\tilde{u}) \Longleftrightarrow u_{i, j} = 
                    \begin{dcases*}
                        \frac{1}{1 + 2\lambda\tau}\tilde{u}_{i, j} & \textnormal{if $|\tilde{u}_{i, j}| \le \sqrt{\frac{\nu}{\lambda}}(1 + 2\lambda\tau)$,} \\
                        \tilde{u}_{i, j} & \textnormal{else.}
                    \end{dcases*}
            \label{eq:proximity_operator_r}
            \end{equation}
        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        Computing the proximity operator for $R_{MS}^{\ast}$ is now straightforward. We use Moreau's Theorem and get
            \begin{eqnarray}
                (\textnormal{Id} + \sigma \partial R_{MS}^{\ast})^{-1}(\tilde{p}) &=& \tilde{p} - \sigma \bigg(\textnormal{Id} + \frac{1}{\sigma} \partial R_{MS}\bigg)^{-1}\big(\frac{\tilde{p}}{\sigma}\big) \notag \\
                &=& \tilde{p} - \sigma
                    \begin{dcases*}
                        \frac{1}{1 + \lambda\frac{1}{\sigma}}\frac{\tilde{p}}{\sigma} & \textnormal{if $|\frac{\tilde{p}}{\sigma}| \le \sqrt{\frac{\nu}{\lambda}}(1 + \lambda\frac{1}{\sigma})$,} \\
                        \frac{\tilde{p}}{\sigma} & \textnormal{else,}
                    \end{dcases*} \notag \\
                &=& \begin{dcases*}
                        \tilde{p} - \frac{\sigma}{\sigma + \lambda}\tilde{p} & \textnormal{if $\frac{1}{\sigma}|\tilde{p}| \le \sqrt{\frac{\nu}{\lambda}}(1 + \lambda\frac{1}{\sigma})$,} \\
                        0 & \textnormal{else,}
                    \end{dcases*} \notag \\
                &=& \begin{dcases*}
                        \frac{\sigma + \lambda}{\sigma + \lambda}\tilde{p} - \frac{\sigma}{\sigma + \lambda}\tilde{p} & \textnormal{if $|\tilde{p}| \le \sqrt{\frac{\nu}{\lambda}}(\sigma + \lambda)$,} \\
                        0 & \textnormal{else,}
                    \end{dcases*} \notag \\
                &=& \begin{dcases*}
                        \frac{\lambda}{\sigma + \lambda}\tilde{p} & \textnormal{if $|\tilde{p}| \le \sqrt{\frac{\nu}{\lambda}}(\lambda + \sigma)$,} \\
                        0 & \textnormal{else,}
                    \end{dcases*} \notag
            \end{eqnarray}
        which is what we obtained before. Since the data fidelity term $G(u) = ||u - g||_{2}^{2}$ is uniformly convex we choose to solve this saddle-point problem with Algorithm \ref{alg:f_star_or_g_uniformly_convex}.

        In this section we were able to provide a framework to solve highly non-convex Mumford-Shah functional. Unfortunately, there is no proof of convergence to the global minimum value. As we will see in Chapter \ref{cha:applications_to_imaging} this lack of correctness does not matter. From a mathematical point of view, we still want to be able to compute the global minimum of the Mumford-Shah functional. In the next chapter we will focus a framework, which again uses the primal-dual algorithm, that solves the functional optimally.
        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    % subsubsection the_proximity_operators_of_the_tvl1_model (end)

% section the_mumford_shah_functional (end)