\subsection{An alternative approach using Lagrange Multiplier}
            Let $f(x, y) := \langle Ax, y \rangle$. Hence we have
                \begin{equation}
                    \min_{x \in C} \max_{y \in K} f(x, y) = \min_{x \in C} \max_{y \in K} \langle Ax, y \rangle. \label{eq:original}
                \end{equation}
            Rewrite non-local-constraint in the set $K$ - \ref{eq:localconst} is pointwise so this is already easy to compute: \\
            We get the formulation
                \begin{equation}
                    ||p_{k_{1}, k_{2}}||_{2} \le \nu \,\,  \textnormal{ s.t. } \, \, p_{k_{1}, k_{2}} = \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} \label{eq:lmnew}
                \end{equation}
            which corresponds to \ref{eq:nonlocalconst}. \\
            The constraint functions in \ref{eq:lmnew} is equivalent to
                \begin{equation}
                    g(p, y) := p_{k_{1}, k_{2}} - \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} = 0 \label{eq:gofp}
                \end{equation}
            holding for all combinations of $k_{1}, k_{2}$ with $1 \le k_{1} \le k \le k_{2} \le M$. \\
            Let $K = \frac{M(M-1)}{2}$ be all possible combinations of the $k_{1}, k_{2}$. For a Lagrange (dual) function $\mathcal{L}$ depending on the optimization problem in \ref{eq:original} we get
                \begin{eqnarray}
                    \mathcal{L}(x, y, \lambda, p) &=& f(x, y) + \sum_{K} \lambda_{K} g(p, y) = \notag \\
                    &&\langle Ax, y \rangle + \sum_{k_{1} = 1}^{M} \sum_{k_{2} = k_{1}}^{M} \langle \lambda_{k_{1}, k_{2}}, p_{k_{1}, k_{2}} - \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} \rangle.
                \end{eqnarray}
            where $x = x(i, j, k) \in \mathbb{R}^{N \times N \times M}, y(i, j, k) = (y_{1}(i, j, k), y_{2}(i, j, k), y_{3}(i, j, k))^{T} \in \mathbb{R}^{3 \times N \times N \times M}, \lambda(i, j, k) \in \mathbb{R}^{2 \times K \times N \times N}$ and $p(i, j, k) \in \mathbb{R}^{2 \times K \times N \times N}$. \\
            Now, instead of minimizing over $x$ and maximizing over $y$ we have an equivalent formulation:
                \begin{equation}
                    \min_{x \in C} \max_{y \in K} f(x, y) \Longleftrightarrow \min_{\substack{x \in C \\ \lambda_{k_{1}, k_{2}}}} \max_{\substack{y \in K_{p} \\ ||p_{k_{1}, k_{2}}|| \le \nu}} \mathcal{L} (x, y, \lambda, p),
                \end{equation}
            where $K_{p}$ denotes the subset of $K$ with only the parabola constraint, i.e.
                \begin{eqnarray}
                    &&K = \{ y = (y^{1}, y^{2}, y^{3})^{T} \in Y: \notag \\
                    &&y^{3}(i,j,k) \ge \frac{y^{1}(i,j,k)^{2} + y^{2}(i,j,k)^{2}}{4} - \lambda(\frac{k}{M} - f(i,j))^{2}.
                \end{eqnarray}

            To solve this saddle-point problem we can again apply our primal dual algorithm. As mentioned we can solve problems of the form $\min \max \langle Ax, y \rangle$ with this algorithm. \\
            Now, let $\tilde{x} = (x, \lambda)^{T}$ and $\tilde{y} = (y, p)^{T}$ then we have
                % \begin{algbox}
                    \begin{algorithm}
                        Choose $(\tilde{x}^{0}, \tilde{y}^{0}) \in \tilde{C} \times \tilde{K}$ and let $\bar{\tilde{x}}^{0} = \tilde{x}^{0}$. We choose $\tau, \sigma > 0$. Then, we let for each $n \ge 0$
                            \begin{equation}
                                \left\{ 
                                    \begin{array}{l l}
                                      \tilde{y}^{n+1} = \Pi_{\tilde{K}} (\tilde{y}^{n} + \sigma A \bar{\tilde{x}}^{n}) \\
                                      \tilde{x}^{n+1} = \Pi_{\tilde{C}} (\tilde{x}^{n} - \tau A^{*} \tilde{y}^{n+1}) \\
                                      \bar{\tilde{x}}^{n+1} = 2\tilde{x}^{n+1} - \tilde{x}^{n}.
                                    \end{array}
                                \right.
                            \end{equation}
                    \end{algorithm}
                % \end{algbox}

            The question how to compute the new projections and to bring the $\lambda$ and $p$ into this algorithm remains. To give the answer to this question we first need to differentiate the Lagrange function, to get the derivative.

                \begin{eqnarray}
                    \frac{\partial \mathcal{L}(x, y, \lambda, p)}{\partial x} &=& A^{T} y \\
                    \frac{\partial \mathcal{L}(x, y, \lambda, p)}{\partial p} &=& \sum_{k_{1}, k_{2}} \lambda_{k_{1}, k_{2}} \\
                    \frac{\partial \mathcal{L}(x, y, \lambda, p)}{\partial \lambda} &=& \sum_{k_{1}, k_{2}} \bigg( p_{k_{1}, k_{2}} - \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} \bigg) \\
                    \frac{\partial \mathcal{L}(x, y, \lambda, p)}{\partial y} &=& Ax + \hat{y}.
                \end{eqnarray}

            To see what $\hat{y}$ looks like, we need to compute the partial derivative of $\mathcal{L}$ in the $y$-direction. By looking at the $l$-th component we observe:

                \begin{eqnarray}
                    \frac{\partial \mathcal{L}(x, y, \lambda, p)}{\partial y_{l}} &=& (Ax)_{l} + \frac{\partial}{\partial y_{l}} \bigg(\sum_{k_{1} = 1}^{M} \sum_{k_{2} = k_{1}}^{M} \langle \lambda_{k_{1}, k_{2}}, p_{k_{1}, k_{2}} - \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} \bigg) \notag \\
                    &=& (Ax)_{l} + \frac{\partial}{\partial y_{l}} \bigg(\sum_{k_{1} = 1}^{M} \sum_{k_{2} = k_{1}}^{M} \langle \lambda_{k_{1}, k_{2}}, p_{k_{1}, k_{2}} \rangle - \langle \lambda_{k_{1}, k_{2}}, \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} \bigg) \notag \\
                    &=& (Ax)_{l} - \frac{\partial}{\partial y_{l}} \bigg(\sum_{k_{1} = 1}^{M} \sum_{k_{2} = k_{1}}^{M} \langle \lambda_{k_{1}, k_{2}}, \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T} \bigg) \notag \\
                    &=& (Ax)_{l} - (\lambda_{l}^{1} + \lambda_{l}^{2})
                \end{eqnarray}

            We observe the following algorithm:
                % \begin{algbox}
                    \begin{algorithm}
                        Choose $(x^{0}, y^{0}, \lambda^{0}, p^{0}) \in C \times K_{p} \times \mathbb{R}^{2 \times N \times N \times M} \times \mathbb{R}^{2 \times N \times N \times M}$ and let $\bar{x}^{0} = x^{0}, \bar{\lambda}^{0} = \lambda^{0}$. We choose $\tau_{x} = \frac{1}{6}, \tau_{\lambda} = \frac{1}{2 + k_{2} - k_{1}}, \sigma_{y} = \frac{1}{3 + M}, \sigma_{p} = 1$. Then, we let for each $n \ge 0$
                            \begin{equation}
                                \left\{ 
                                    \begin{array}{l l}
                                      y^{n+1} = \Pi_{K_{p}} (y^{n} + \sigma_{y} (A \bar{x}^{n} + \tilde{y})) \\
                                      p_{k_{1}, k_{2}}^{n+1} = \Pi_{||\cdot||_{2} \le \nu} (p_{k_{1}, k_{2}}^{n} + \sigma_{p} \bar{\lambda}_{k_{1}, k_{2}}^{n}) \\
                                      x^{n+1} = \Pi_{C} (x^{n} - \tau_{x} A^{*} y^{n+1}) \\
                                      \lambda_{k_{1}, k_{2}}^{n+1} = \lambda_{k_{1}, k_{2}}^{n} - \tau_{\lambda} (p_{k_{1}, k_{2}}^{n+1} - \sum_{k_{1} \le k \le k_{2}} (y_{1}(i, j, k), y_{2}(i, j, k))^{T}) \\
                                      \bar{x}^{n+1} = 2x^{n+1} - x^{n} \\
                                      \bar{\lambda}_{k_{1}, k_{2}}^{n+1} = 2\lambda_{k_{1}, k_{2}}^{n+1} - \lambda_{k_{1}, k_{2}}^{n}.
                                    \end{array}
                                \right.
                            \end{equation}
                    \end{algorithm}
                % \end{algbox}

% chapter an_algorithm_for_minimizing_the_mumford_shah_functional (end)