\section{The Mumford-Shah Functional} % (fold)
\label{sec:the_mumford_shah_functional}
    
    We saw that solving the ROF Model and TVL1 Model, respectively, depended mainly on finding the right choices of the convex conjugates and the proximity operator. To compute these, we applied some basic mathematical rules and used the convexity of our problems. Now, we introduce a highly non-convex functional which is much more difficult to solve. The problem of the result in this section is, that there is - to date - no proof that it convergences to the global minimum of the corresponding minimization problem. Even though, one can see, in the chapter where we discuss applications for our models, that it yields to high quality solutions. But, of course one wants to be sure to have the global optimum and for that a good solution. Therefore, we discuss in the next chapter a method to minimize the Mumford-Shah fucntional which proposes then optimal extrema. For now, let us first give the definition of this functional.

    \begin{definition}[The Mumford-Shah Functional \cite{Strekalovskiy-Cremers-eccv14}] % (fold)
    \label{def:the_mumford_shah_functional}

        Let $\Omega \subset \mathbb{R}^{2}$ be a rectangular image domain. In order to approximate an input image $g: \Omega \longrightarrow \mathbb{R}$ in terms of a piecewise smooth function $u: \Omega \longrightarrow \mathbb{R}$, the Mumford-Shah Functional is given by
                
                \begin{equation}
                    E(u) = \frac{1}{2} \int_{\Omega} (u - g)^{2} dx + R(u) = \frac{1}{2} \int_{\Omega} (u - g)^{2} dx + \frac{1}{2} \alpha \int_{\Omega \setminus K} |\nabla u|^{2} dx + \lambda |K|,
                \end{equation}
                \label{eq:the_mumford_shah_functional}
            
            where $\lambda, \alpha > 0$ are weighting parameters, $K = K_{1} \cup ... \cup K_{N}$ and $|K|$ denotes the length of the curves in $K$.

    \end{definition}
    % definition the_mumford_shah_functional (end)

    \begin{remark}
        In this section we follow the represenation of the Mumford-Shah Functional of \cite{Strekalovskiy-Cremers-eccv14}. This is totally equivalent to Equation \ref{eq:the_mumford_shah_functional}, but has some advantages for our notation. In Chapter \ref{cha:a_first_order_primal_dual_algorithm_for_minimizing_the_mumford_shah_functional} we will state another notation, which is totally consistent.
    \end{remark}

    This functional differs from the ones in the previous sections. The first term, which is again the data fidelity term, remains the same as in the ROF or TVL1 Model respectively. The approximation $u$ should be as close enough to the input image $g$ as possible. Therefore, the quadratic distance measure is used. The regularizer consists of two terms. The first term uses again the gradient, but not in the set $\Omega$ itself. Instead it states that the approximation $u$ is not allowed to change too much in sets $\Omega \setminus K_{i}$. We call $K_{i}$ the discontinouty sets (or jump sets) for all $i = 1, ..., N$ and curves $u_{i}$. The gradient is only taken into account in regions smooth enough. These discontinouties are also measured and taken into account in our energy $E(u)$. It is the last term, which penalizes the length of the curves $K_{i}$. This means, that the curves should be regular in the sense of measure theory. Here, we also find two weighting parameters $\lambda$ and $\alpha$. Where $\alpha$ handles the tradeoff between the first two terms as in the ROF or TVL1 Model, $\lambda$ controls the length of the discontinouty sets. A smaller $\lambda$ yields to a smoother image, where a higher $\lambda$ leads to sharper edges in our images. The parameter $\alpha$ yet plays another important role. If one chooses a $\alpha$ small enough, then our model is also called piecewise-smooth Mumford-Shah Model. On the other hand, in the limiting case $\alpha \longrightarrow \infty$, we can only attain a minimum if we set $\nabla u = 0$ in $\Omega \setminus K$. Then the model is known as the piecewise-constant Mumford-Shah Model.
    In \cite{Strekalovskiy-Cremers-eccv14} Strekalovskiy and Cremers proposed to rewrite this functional in a discrete setting by first defining the discrete regularizer function by
        \begin{equation}
            R_{MS}(u) = \min(\frac{\alpha}{2}||u||_{2}^{2},\lambda).
        \label{eq:ms_regularizer}
        \end{equation}
    Then the discrete Mumford-Shah Model can be expressed by
        \begin{equation}
            \min_{u \in X} E_{MS}(u) = \min_{u \in X} \frac{1}{2} ||u - g||_{2}^{2} + R_{MS}(\nabla u).
        \label{eq:discrete_mumford_shah_functional}
        \end{equation}
    According to \cite{Strekalovskiy-Cremers-eccv14} the idea behind this formulation is to model the discontinouty set $K$ explicitly in terms of the function $u$. This means, that $K$ is the set of all points where the minimum in \ref{eq:ms_regularizer} attains $\lambda$. In other words, if the gradient $\nabla u$ is large enough we have for the explicit set $K_{MS}$:
        \begin{equation}
            K_{MS} = \bigg\{ (i, j) \in \Omega : ||\nabla u_{i, j}||_{2}^{2} \ge \sqrt{2 \frac{\lambda}{\alpha}} \bigg\}.
        \label{eq:set_k_ms}
        \end{equation}
    One checks easy that for a point $(i, j) \in K_{MS}$ we observe
        $$
            R_{MS}(\nabla u_{i, j}) = \min(\underbrace{\frac{\alpha}{2}||\nabla u_{i, j}||_{2}^{2}}_{\ge \frac{\alpha}{2} \sqrt{2 \frac{\lambda}{\alpha}}^{2} = \lambda}, \lambda) = \lambda
        $$
    and if $(i, j) \notin K_{MS}$ we have
        $$
            R_{MS}(\nabla u_{i, j}) = \min(\underbrace{\frac{\alpha}{2}||\nabla u_{i, j}||_{2}^{2}}_{< \frac{\alpha}{2} \sqrt{\frac{2 \lambda}{\alpha}}^{2} = \lambda}, \lambda) = \alpha||\nabla u_{i, j}||_{2}^{2}.
        $$

    \begin{remark}
        In the piecewise-constant case (also known as the cartoon limit) where $\alpha \longrightarrow \infty$, Equation \ref{eq:ms_regularizer} changes to
            \begin{equation}
                R_{MS}(u) = 
                    \begin{dcases*}
                        \lambda & \textnormal{if $u \ne 0$,} \\
                        0 & \textnormal{else}.
                    \end{dcases*}
            \label{eq:ms_regularizer_piecewise_constant}
            \end{equation}
    \end{remark}

    \subsection{Mumford-Shah as Saddle-Point Problem} % (fold)
    \label{sub:mumford_shah_as_saddle_point_problem}

        Again, we try to formulate the Mumford-Shah Model as a saddle-point problem to be abble to apply one of the primal-dual algorithms of Section \ref{sec:a_firs_order_primal_dual_algorithm}. In the sense of our notations from the previous section we have
            \begin{equation}
                \min_{u \in X}\,\, F(\nabla u) + G(u) = R_{MS}(\nabla u) + \frac{1}{2} ||u - g||_{2}^{2}
                \label{eq:primal_mumford_shah_model}
            \end{equation}
        This is the primal formulation for the discrete Mumford-Shah Model. Applying now the Legendre-Fenchel conjugate on the Mumford-Shah regularizer $R_{MS}$ we get the primal-dual formulation with
            \begin{equation}
                \min_{u \in X}\, \max_{p \in Y}\,\, \langle p, \nabla \, u \rangle_{X} - R_{MS}^{\ast}(p) + G(u) = \min_{u \in X}\, \max_{p \in Y}\,\, -\langle \nabla^{T}\,p, u \rangle_{X} - R_{MS}^{\ast}(p) + G(u).
            \label{eq:primal_dual_mumford_shah_model}
            \end{equation}
        As before, we want to compute the convex conjugate of $R_{MS}$. We have
            $$
                R_{MS}^{\ast}(p) = \sup_{u \in X} \langle p, u \rangle - R_{MS}(u) = \sup_{p \in Y} \langle p, u \rangle - \min(\alpha ||u||_{2}^{2}, \lambda).
            $$
        We need to distinguish both cases of our minimum function.
            \begin{enumerate}
                \item Assume that $\min(\frac{\alpha}{2} ||u||_{2}^{2}, \lambda) = \lambda$. We get
                    $$
                        \sup_{u \in X} \langle p, u \rangle - \lambda = S_{Y}(p) - \lambda,
                    $$
                where $S_{Y}(p)$ denotes the support function of $Y$ for a point $p \in Y$. Clearly, if $p \neq = 0$, then the supremum over all $u$ is $\infty$. But if $p = 0$, this expression becomes
                    $$
                        S_{Y}(0) - \lambda = - \lambda.
                    $$
                \item On the other side if we assume that $\min(\frac{\alpha}{2} ||u||_{2}^{2}, \lambda) = \frac{\alpha}{2} ||u||_{2}^{2}$ we can apply Example \ref{ex:legendre_fenchel_conjugate_example} 2. and 4. Let $\hat{R}(u) = \frac{||u||_{2}^{2}}{2}$, then the Legendre-Fenchel conjugate of $\alpha \hat{R}(u)$ is $\alpha \hat{R}^{\ast}(\frac{p}{\alpha})$. In the end we observe
                    \begin{equation}
                        R_{MS}^{\ast}(p) = \alpha \hat{R}^{\ast}(\frac{p}{\alpha}) = \frac{||p||_{2}^{2}}{2\alpha}.
                    \label{eq:mumford_shah_convex_conjugate}
                    \end{equation}
                If we now make use of Proposition \ref{prop:convex_subgradient}, we have that $p \in \partial R_{MS}(u)$ if and only if
                    $$
                        \langle p, u \rangle - R_{MS}(u) = R_{MS}^{\ast}(p).
                    $$
                First, let us verify that this expression holds.
                    \begin{proof}
                        "$\Longrightarrow$":\\
                        Let $p \in \partial R_{MS}(u) = \alpha u$, then this is equivalent to $p = \alpha u \Longleftrightarrow u = \frac{p}{\alpha}$. Pluging this into the equation we get
                            \begin{eqnarray}
                                \langle p, u \rangle - R_{MS}(u) &=& \langle p, \frac{p}{\alpha} \rangle - \frac{\alpha}{2}||\frac{p}{\alpha}||_{2}^{2} = \frac{1}{\alpha} ||p||_{2}^{2} - \frac{\alpha}{2 \alpha^{2}} ||p||_{2}^{2} \notag \\
                                &=& \frac{1}{\alpha} ||p||_{2}^{2} - \frac{1}{2 \alpha} ||p||_{2}^{2} \notag \\
                                &=& \frac{1}{\alpha} ||p||_{2}^{2} = R_{MS}^{\ast}(p).
                            \end{eqnarray}
                        "$\Longleftarrow$":\\
                        Now, rewrite the equation to
                            $$
                                R_{MS}(u) = \langle p, u \rangle - R_{MS}^{\ast}(p).
                            $$
                        Taking the subdifferential on $R$ we observe
                            $$
                                \partial R_{MS}(u) = \partial \bigg( \langle p, u \rangle - R_{MS}^{\ast}(p) \bigg) \Longleftrightarrow \alpha u = p \Longleftrightarrow p \in \alpha u = \partial R_{MS}(u).
                            $$
                        \qed
                    \end{proof}
                Knowing that $p \in \partial R_{MS}(u)$ or $u = \frac{p}{\alpha}$ respectively, we have that Equation \ref{eq:mumford_shah_convex_conjugate} only holds if $\frac{\alpha}{2} ||u||_{2}^{2} \le \lambda$ or if $\frac{\alpha}{2} ||\frac{p}{\alpha}||_{2}^{2} = \frac{||p||_{2}^{2}}{2\alpha} \le \lambda$ or equivalently $||p||_{2} \le \sqrt{2 \alpha \lambda}$.
            \end{enumerate}
        Overall, the convex conjugate of the Mumford-Shah regularizer is given by
            \begin{equation}
                R_{MS}^{\ast}(p) =
                    \begin{dcases*}
                        \frac{||p||_{2}^{2}}{2\alpha} & \textnormal{if $||p||_{2} \le \sqrt{2 \alpha \lambda}$,} \\
                        \infty & \textnormal{else.}
                    \end{dcases*}
            \end{equation}
        To derive the dual formulation of the Mumford-Shah Model we would just need to plug the derived definition of the convex conjugate into the dual problem.
            \begin{equation}
                \max_{p \in Y} - (G^{\ast}(-K^{\ast}p) + R_{MS}^{\ast}(p)) = \max_{p \in Y} -(||\nabla^{T}p - g||_{2}^{2}) + \frac{||p||_{2}^{2}}{2\alpha},
            \label{eq:dual_mumford_shah_model}
            \end{equation}
        as long as $||p||_{2} \le \sqrt{2 \alpha \lambda}$. If this inequality does not hold we have that
            $$
                \max_{p \in Y} - (G^{\ast}(-K^{\ast}p) + R_{MS}^{\ast}(p)) = \max_{p \in Y} -(||\nabla^{T}p - g||_{2}^{2}) + \infty = \infty.
            $$
        We are know interested in the proximity operators for our model, which we compute in the next subsection.

        % Then, if $p \neq 0$, the only choice for $R_{MS}^{\ast}$ would be $\infty$. This is because if $p < 0$ we get as $u \longrightarrow -\infty$ that $\sup\limits_{u \in X} \langle p, u \rangle - \lambda = \infty$. If $p > 0$ and $u \longrightarrow \infty$ we obtain the same result. On the other hand if $\min(\alpha ||u||_{2}^{2}, \lambda) = \alpha ||u||_{2}^{2}$ then we can use Example \ref{ex:legendre_fenchel_conjugate_example} 2. and 4. For that we also note that $\lambda > 0$ for that $\alpha ||u||_{2}^{2} \in (0, \lambda)$. If we set $\hat{R}(u) = \frac{||u||_{2}^{2}}{2}$ and $R_{MS}(u) = \alpha \hat{R}(u)$. Then the Legendre-Fenchel conjugate is given by
        %     $$
        %         R_{MS}^{\ast}(p) = \alpha \hat{R}^{\ast}(\frac{p}{\alpha}) = \frac{1}{\alpha} \frac{||p||_{2}^{2}}{2}.
        %     $$
        % Overall we obtain
        %     \begin{equation}
        %         R_{MS}^{\ast}(p) =
        %             \begin{dcases*}
        %                 \frac{1}{\alpha} \frac{||p||_{2}^{2}}{2} & \textnormal{if $||p||_{2}^{2} > 2 \frac{\lambda}{\alpha}$,} \\
        %                 -\lambda & \textnormal{if $p = 0$,} \\
        %                 \infty & \textnormal{if $||p||_{2}^{2} \le 2 \frac{\lambda}{\alpha}$.}
        %             \end{dcases*}
        %     \label{eq:mumford_shah_convex_conjugate}
        %     \end{equation}
        % Using that $G^{\ast}(p) = \frac{\lambda}{2}||p - g||_{2}^{2}$, c.f. Subsection \ref{sub:rof_model_as_saddle_point_problem}, we obtain the dual problem of the Mumford-Shah Model by
        %     \begin{equation}
        %         \max_{p \in Y} -(G^{\ast}(-K^{\ast}p) + R_{MS}^{\ast}(p)) = \max_{p \in Y} - (\frac{\lambda}{2}||\nabla^{T}p - g||_{2}^{2}) + ...
        %     \label{eq:dual_mumford_shah_model}
        %     \end{equation}
    
    % subsection mumford_shah_as_saddle_point_problem (end)

    \subsection{The Proximity Operators of the Mumford-Shah Model} % (fold)
    \label{sub:the_proximity_operators_of_the_mumford_shah_model}
        
        Let us now compute the proximity operators of our proposed model. The operator for the function $G$ can be adapted from Equation \ref{eq:proximity_operator_g_rof}, because our data fidelity term is the same as in the ROF Model. Then we have pointwise for all $i = 1, ..., N$ and $j = 1, ..., M$

            \begin{equation}
                (\textnormal{Id} + \tau\,\partial\,G)^{-1}(\tilde{u}) = u \Longleftrightarrow u_{i,j} = \frac{\tilde{u}_{i,j} + \tau\lambda g}{1 + \tau\sigma}
            \label{eq:proximity_operator_g_mumford_shah}
            \end{equation}




    % subsection the_proximity_operators_of_the_mumford_shah_model (end)

    % \subsubsection{The Proximity Operators of the TVL1 Model} % (fold)
    % \label{ssub:the_proximity_operators_of_the_tvl1_model}
        
    %     For the implementation of the Primal-Dual Algorithm we need the proximity operators of the TVL1 Model. Fortunatelly, the operator for the function $F^{\ast}$ remains the same as in Subsection \ref{sub:the_proximity_operators_for_the_rof_model} since the functions are the same. We get again
    %         \begin{equation}
    %             (\textnormal{Id} + \sigma\,\partial\,F^{\ast})^{-1}(\tilde{p}) = P_{l_{2}}(\tilde{p}) = p \Longleftrightarrow p_{i,j} \frac{\tilde{p}_{i, j}}{\max(1, |\tilde{p}_{i, j}|)},
    %         \label{eq:proximity_operator_f_star_tvl1}
    %         \end{equation}
    %     for all $i = 1, ..., N, j = 1, ..., M$.

    %     To compute the proximity operator of the function $G$ we apply again Definition \ref{eq:proximity_operator}. Then we have
    %         $$
    %             (\textnormal{Id} + \tau\,\partial\,G)^{-1}(\tilde{u}) = \min_{u \in X} \frac{||u - \tilde{u}||_{2}^{2}}{2} + \lambda\tau||u - g||_{1}.
    %         $$
    %     We already saw that the $l^{1}$ norm is not differentiable everywhere and for that non-smooth. But we obtained in Example \ref{ex:subgradient_subdifferential} 2. that the subgradient $y$ of the $l^{1}$ norm is given by
    %         $$
    %             y =
    %                 \begin{dcases*}
    %                     1 & \textnormal{if $u > 0$,} \\
    %                     -1 & \textnormal{if $u < 0$,} \\
    %                     -1 \,\textnormal{or}\, 1 & \textnormal{if $u = 0$.}
    %                 \end{dcases*}
    %         $$
    %     One idea to reformulate $y$ is to use the signum function. The subgradient is then given by
    %         $$
    %             y =
    %                 \begin{dcases*}
    %                     1 \cdot \textnormal{sgn}(u) & \textnormal{if $u \ne 0$,} \\
    %                     0 & \textnormal{if $u = 0$.}
    %                 \end{dcases*}
    %         $$
    %     Solving the minimization problem of the proximity operator is then quite easy. We set $\mathcal{L}(u) = \frac{||u - \tilde{u}||_{2}^{2}}{2} + \lambda\tau||u||_{1}$ and compute $\nabla \mathcal{L}(u) = 0$. We have
    %         $$
    %             \nabla \mathcal{L}(u) = u - \tilde{u} + \lambda 
    %                 \begin{dcases*}
    %                     1 \cdot \textnormal{sgn}(u) & \textnormal{if $u \ne 0$,} \\
    %                     0 & \textnormal{if $u = 0$.}
    %                 \end{dcases*}
    %         $$

    %     Overall, we have pointwise
    %         \begin{equation}
    %             u = (\textnormal{Id} + \tau\,\partial\,G)^{-1}(\tilde{u}) \Longleftrightarrow u_{i, j} = 
    %                 \begin{dcases*}
    %                     \tilde{u}_{i,j} - \tau\lambda & \textnormal{if $\tilde{u}_{i,j} - g_{i,j} > \tau\lambda$,} \\
    %                     \tilde{u}_{i,j} + \tau\lambda & \textnormal{if $\tilde{u}_{i,j} - g_{i,j} < - \tau\lambda$,} \\
    %                     g_{i, j} & \textnormal{if $|\tilde{u}_{i,j} - g_{i,j}| \le \tau\lambda$,}.
    %                 \end{dcases*}
    %         \label{eq:prox_g_tvl1}
    %         \end{equation}

    % subsubsection the_proximity_operators_of_the_tvl1_model (end)

% section the_mumford_shah_functional (end)