\section{The TVL1 Model} % (fold)
\label{sec:the_tvl1_model}
    
    The idea within the TVL1 model is to replace the $L_{2}$ norm in the data fidelity term with the $L^{1}$ norm. This norm is more robust in removing the so called salt and pepper noise, meaning that it removes single pixels with extrem values white or black. The model is depicted as follows:

    \begin{definition}[The TVL1 Model] % (fold)
    \label{def:the_tvl1_model}

        Let $\Omega \in \mathbb{R}^{d}$ be the $d$-dimensional image domain, $u \in W_{1}^{1}(\Omega)$ and $g \in L^{1}(\Omega)$ a (noisy) input image. Then the TVL1 Model is defined as the variational problem
            \begin{equation}
                \min_{u} E_{TVL1}(u) = \min_{u} \textnormal{TV}(u) + \lambda \int_{\Omega} |u - g| \, dx = \min_{u \in X} \int_{\Omega} |\nabla \, u| \, dx + \lambda \int_{\Omega} |u - g| \, dx.
                \label{eq:the_tvl1_model}
            \end{equation}

    \end{definition}
    % definition the_tvl1_model (end)

    Note, that there is also the parameter $\lambda$ to handle the tradeoff between both terms, as seen in the previous section for the ROF model. Then, primal formulation of the TVL1 model is represented by
        \begin{equation}
            \min_{u \in X} E_{TVL1}(u) = \min_{u \in X} ||\nabla \, u||_{1} + \lambda ||u - g||_{1},
        \label{eq:primal_tvl1_problem}
        \end{equation}
    where $u$ is the approximation of an input image $g$.

    \subsection{TVL1 as Saddle-Point Problem} % (fold)
    \label{sub:tvl1_as_saddle_point_problem}

        We can rewrite this minimization problem into a saddle-point problem. Let us first state, that our function $F(\nabla u)$ remains the same as in the ROF model. We only have a change in $G$.
        %Then the primal problem of the TVL1 model becomes
            % \begin{equation}
            %     \min_{u \in X}\,\, F(\nabla u) + G(u) = \min_{u \in X}\,\, ||\nabla \, u||_{1} + \lambda ||u - g||_{1}.
            %     \label{eq:primal_tvl1_problem}
            % \end{equation}
        Again using the Legendre-Fenchel conjugate for $F$ - as in subsection \ref{sub:rof_model_as_saddle_point_problem} - we obtain
            $$
                \min_{u \in X}\, \max_{p \in Y}\,\, \langle p, \nabla \, u \rangle_{X} - F^{\ast}(p) + G(u)% = \min_{u \in X}\, \max_{p \in Y}\,\, -\langle \nabla^{T}\,p, u \rangle_{X} - F^{\ast}(p) + G(u).
            $$
        From equation \ref{eq:rof_f_star} we already know how $F^{\ast}$ looks like. Writing it as the indicator function we got $F^{\ast} = \delta_{P}(p)$ with the set $P$ of Equation \ref{eq:the_set_P}. Then the primal-dual problem becomes
            \begin{equation}
                \min_{u \in X}\, \max_{p \in Y}\,\, \langle p, \nabla\, u \rangle_{Y} + \lambda ||u - g||_{1} - \delta_{P}(p).
            \label{eq:primal_dual_tvl1_problem}
            \end{equation}
        To give the representation of the dual-problem for the TVL1 model, we need to compute $G^{\ast}$. We find, that the Legendre-Fenchel conjugate of the function $G(u) = \lambda||u-g||_{1}$ is given by
            \begin{equation}
                G^{\ast}(q) =
                    \begin{dcases*}
                        \langle q, g \rangle & \textnormal{if $||q||_{\infty} \le \lambda$,} \\
                        \infty & \textnormal{else},
                    \end{dcases*}
                \label{eq:tvl1_g_star}
            \end{equation}
        which means nothing but $G^{\ast}(q) = \delta_{Q}(q) + \langle q, g \rangle$ for a set
            \begin{equation}
                Q = \big\{ q \in X : ||q||_{\infty} \le \lambda \big\}.
            \label{eq:the_set_Q}
            \end{equation}

            \begin{proof}
                To derive this representation of the conjugate function we set $z = u - g$, which is equivalent to $u = z + g$. Then with the definition of the Legendre-Fenchel conjugate we get
                    \begin{eqnarray}
                        G^{\ast}(q) = \sup_{u \in X} \langle q, u \rangle - G(u) \Longleftrightarrow G^{\ast}(q) &=& \sup_{z \in X} \langle q, z + g \rangle - G(z + g) \notag \\
                        &=& \sup_{z \in X} \langle q, z + g \rangle - \lambda ||z||_{1} \notag \\
                        &=& \sup_{z \in X} \frac{1}{\lambda} \big(\langle q, z \rangle + \underbrace{\langle q, g \rangle}_{= \textnormal{const}} \big) - ||z||_{1} \notag \\
                        &=& \bigg(\sup_{z \in X} \langle \frac{1}{\lambda} q, z \rangle - ||z||_{1} \bigg) + \langle q, g \rangle.
                    \end{eqnarray}
                Using again example \ref{ex:legendre_fenchel_conjugate_example} 2. and the fact that the conjugate norm of the $l_{1}$ norm is the $l_{\infty}$ norm, gives us
                    % Since we are facing to compute the Legendre-Fenchel conjugate of the $l^{1}$ norm we already know from Example \ref{ex:legendre_fenchel_conjugate_example} 2. that
                    $$
                        G^{\ast}(q) =
                            \begin{dcases*}
                                \langle q, g \rangle & \textnormal{if $||\frac{1}{\lambda}q||_{\infty} \le 1$,} \\
                                \infty & \textnormal{else},
                            \end{dcases*} \Longleftrightarrow
                        G^{\ast}(q) =
                            \begin{dcases*}
                                \langle q, g \rangle & \textnormal{if $||q||_{\infty} \le \lambda$,} \\
                                \infty & \textnormal{else}.
                            \end{dcases*}
                    $$
                Or equivalently $G^{\ast}(q) = \delta_{Q}(q) + \langle q, g \rangle$.\qed
            \end{proof}
        We obtain the dual formulation of the TVL1 Model as
            \begin{equation}
                \max_{p \in Y}\,\, -(G^{\ast}(-K^{\ast}p) + F^{\ast}(p)) = \max_{p \in Y} -\bigg( \delta_{Q}(\nabla^{T}\,q) + \langle \nabla^{T}q, g \rangle + \delta_{P}(p) \bigg).
            \label{eq:dual_tvl1_problem}
            \end{equation}
    
    % subsection tvl1_as_saddle_point_problem (end)

    \subsubsection{The Proximity Operators of the TVL1 Model} % (fold)
    \label{ssub:the_proximity_operators_of_the_tvl1_model}
        
        %For the implementation of the Primal-Dual Algorithm we need the proximity operators of the TVL1 Model.
        The proximity operator of $F^{\ast}$ remains the same as in subsection \ref{sub:the_proximity_operators_for_the_rof_model}. We get again
            \begin{equation}
                p = (\textnormal{Id} + \sigma\,\partial\,F^{\ast})^{-1}(\tilde{p}) = \Pi_{P}(\tilde{p}) \Longleftrightarrow p_{i,j} = \frac{\tilde{p}_{i, j}}{\max(1, |\tilde{p}_{i, j}|)},
            \label{eq:proximity_operator_f_star_tvl1}
            \end{equation}
        for all $i = 1, ..., N, j = 1, ..., M$.

        To compute the proximity operator of the function $G$ we get
            $$
                (\textnormal{Id} + \tau\,\partial\,G)^{-1}(\tilde{u}) = \min_{u \in X} \frac{||u - \tilde{u}||_{2}^{2}}{2} + \lambda\tau||u - g||_{1}.
            $$
        If we define $\mathcal{L}(u) = \frac{||u - \tilde{u}||_{2}^{2}}{2} + \lambda\tau||u||_{1}$ and use the linearized notation for the image $u$, namely $u \in \mathbb{R}^{N \cdot M}$, then the minimization problem $\min\limits_{u \in X} \mathcal{L}(u)$ is equivalent to $\nabla\mathcal{L}(u) = 0$. We already saw that the $l^{1}$ norm is not differentiable everywhere and for that non-smooth. To compute the gradient we need the partial derivatives for each $k = 1, ..., N \cdot M$. Let us first take a look at the k-th row of $\nabla\mathcal{L}(u)$:
            \begin{eqnarray}
                \partial_{k}(\mathcal{L}(u)) &=& u_{k} - \tilde{u}_{k} + \tau\lambda \partial_{k}(||u-g||_{1}) \notag \\
                &=& u_{k} - \tilde{u}_{k} + \tau\lambda \partial_{k}(|u_{1}-g_{1}| + ... + |u_{k}-g_{k}| + ... + |u_{n}-g_{n}|) \notag \\
                &=& u_{k} - \tilde{u}_{k} + \tau \lambda \partial_{k}\big(|u_{k} - g_{k}|\big).
            \end{eqnarray}
        With that we see that we need to compute the subgradient of the absolute value function for all partial subderivatives $k = 1, ..., N \cdot M$. From example \ref{ex:subgradient_subdifferential} 1. we have
            $$
                y_{k} =
                    \begin{dcases*}
                        1 & \textnormal{if $u_{k} - g_{k} > 0$,} \\
                        -1 & \textnormal{if $u_{k} - g_{k} < 0$,} \\
                        [-1, 1] & \textnormal{if $u_{k} - g_{k} = 0 \Longleftrightarrow u_{k} = g_{k}$,}
                    \end{dcases*}
            $$
        where $y_{k}$ is the subgradient of the k-th row. Let us check all three cases:
            \begin{enumerate}
                \item Let $y_{k} = 1$. Then we obtain
                    $$
                        u_{k} - \tilde{u}_{k} + \tau\lambda = 0 \Longleftrightarrow u_{k} = \tilde{u}_{k} - \tau\lambda.
                    $$
                With $u_{k} - g_{k} > 0$ and $u_{k} = \tilde{u}_{k} - \tau\lambda$ we have that this equation holds if $\tilde{u}_{k} - \tau\lambda - g_{k} > 0$, which is then equivalent to $\tilde{u}_{k} - g_{k} > \tau\lambda$.
                \item Now, let $y_{k} = -1$. The we get in row k
                    $$
                        u_{k} - \tilde{u}_{k} - \tau\lambda = 0 \Longleftrightarrow u_{k} = \tilde{u}_{k} + \tau\lambda.
                    $$
                Rewriting the constraint in the definition of the subgradient leads us to $\tilde{u}_{k} + \tau\lambda - g_{k} > 0$. From this it follows $\tilde{u} - g_{k} > \tau\lambda$.
                \item Finally, $y_{k} \in [-1, 1]$. Since, we know that $u_{k} = g_{k}$ we get
                    $$
                        g_{k} - \tilde{u}_{k} + \tau\lambda y_{k} = 0 \Longleftrightarrow \tilde{u}_{k} - g_{k} = \tau\lambda y_{k}.
                    $$
                We apply the absolute value function to each side of the equation, then with $|y_{k}| \le 1$ we have
                    $$
                        |\tilde{u}_{k} - g_{k}| = |\tau\lambda y_{k}| \le \tau\lambda.
                    $$
            \end{enumerate}
        Overall, we observe for all $i = 1, ..., N$, $j = 1, ..., M$
            \begin{equation}
                u = (\textnormal{Id} + \tau\,\partial\,G)^{-1}(\tilde{u}) \Longleftrightarrow u_{i, j} = 
                    \begin{dcases*}
                        \tilde{u}_{i,j} - \tau\lambda & \textnormal{if\, $\tilde{u}_{i,j} - g_{i,j} > \tau\lambda$,} \\
                        \tilde{u}_{i,j} + \tau\lambda & \textnormal{if\, $\tilde{u}_{i,j} - g_{i,j} < - \tau\lambda$,} \\
                        g_{i, j} & \textnormal{if\, $|\tilde{u}_{i,j} - g_{i,j}| \le \tau\lambda$}.
                    \end{dcases*}
            \label{eq:prox_g_tvl1}
            \end{equation}
        Note, that we used the discrete locations of the pixel $(i, j)$ in this representation instead of the row-wise notation $k$. As we discussed earlier, these two notations are totally consistent.

    % subsubsection the_proximity_operators_of_the_tvl1_model (end)

% section the_tvl1_model (end)