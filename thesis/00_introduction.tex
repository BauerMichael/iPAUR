\chapter{Introduction} % (fold)
\label{cha:introduction}

    In the second decade of the twenty-first century autonomous driving seems to be the big thing for the Car, Computer and Software industry. The expectation is nothing less than having fewer, or even no, accidents. With these cars, industry is trying to change the world to a better. As this may comfort many people, it poses a lot of problems which need to be solved, for instance a stable hardware or even more important viable software. One field of research - of many others - is called Machine Learning, where the computer is trained to make the right decisions in the right situations. The car should be able to accommodate to all possible circumstances which could appear during a drive. To make it even more complicated, the car would need to take a decision in real-time. As one can imagine, it is extremely difficult to develop fast, stable and tractable methods.\\
    Learning from a certain situation, the car and the computer respectively needs data from the environment. This can be the shape of another car, traffic lights, differences in the lighting conditions or the distance to other objects. What all these information have in common, they can be collected via images. Small cameras are tracking the traffic and surrounding and providing data. Using the images, the car's computer can be trained when to stop or where to turn right.\\
    But it's not only the autonomously driving cars which make use of images to learn. Doctors use X-ray view or MRI scans in patient treatment, semiconductor companies use pictures of wafers seeking for damages on it, e.g. scratches or dark spots and unfortunately images are also used in modern warfare. For this reason image processing has become more important during the last decades. Researchers dealt with many problems like edge detection, deblurring, denoising, inpainting or image segmentation. In the field of edge detection the probably most famous researcher is John F. Canny. He provided the first tractable and stable edge detection algorithm, presented in the year 1986 (reference). One step in Canny's edge detector is to denoise and smooth the image. Therefore a Gaussian-Filter is used. The idea behind this filter is to convolute an image with a Gaussian kernel, or also called Gaussian curve. This filter provides reasonable results, but no more no less.\\
    Only three years after Canny published his work, two researchers, namely David Mumford and Jayant Shah, published a paper whose impact continous to this day - to date it was cited over 4.500 times (scholar). They proposed to minimize the energy of the so called Mumford-Shah Functional in order to approximate an image optimally. Solving this optimization problem leads to applications like image denoising, inpainting and even segmentation could be handled. Unfortunately, this functional is by definition non-convex. For this reason finding the minimal energy of it is a NP-hard problem. This fact makes it so difficult to deal with but also so interesting.\\
    One approach to compute the minimizer is by convex relaxation. It makes the non-convex problem convex and one can apply a fast and tractable algorithm. The idea of convex relaxation goes back to Bouchite, Alberti, Dal Maso and was then used and further developed by Thomas Pock et.al. to make use of a fast primal-dual algorithm. It leads us to almost exact solutions and is highly parallelizable. %This algorithm is not only useful in this work. It appears to be the solution of a larger class of problems\\
    Possibly inspired by the Mumford-Shah Functional another idea for image processing took place in 1992. Rudin, Osher and Fatemi focused on total variation based imaging. The total variation is a concept developed in the 19th century. Today there is a large community which associates total variation almost all with image processing. What Rudin et. al. proposed was also a minimization problem, namely the $TVL2$-Model (or $ROF$-Model). As the name suggests it is based on the total variation and the $L2$-Norm. The formulation of it, as we will see, looks quite similar to the Mumford-Shah Functional, but has one term left and is by definition convex. This makes it much easier to handle all computations but the output images are not even as good as they are applying Mumford-Shah. Applications which arise from the $ROF$-Model are rare. Removing noise from a given input image is the most common use. But as one later sees: Solutions are better than using a Gaussian-Filter.\\
    By replacing the $L2$-Norm with the $L1$-Norm in the $ROF$-Model one derives the $TVL1$-Model, hence the name. A convex model and minimization its energy again is quite easy to compute. In this work we will see, that it is possibly the best model of the presented. It can deal with - so called - salt and pepper noise, handles inpainting, output images are sharp and close to the original image and most important: it runs in parallel on a GPU in real-time.\\
    This work will present all of these methods, clarify which properties they have and make all computations clear. We also want to present the primal-dual algorithm which can be used to solve all models. We provide a large range of applications and the underlying run-times. At the end of this work we want to compare the models and conclude which one is the exactest, fastest and most tractable one.
%      We will consider this method in this work and will also talk about it detail. We will deduce and proof all steps wh% This technique was used by a group of researchers - Thomas Pock, Horst Bischof, Antonin Chambolle and Daniel Cremers - who totally changed the  % Football managers analyze their teams - and the opponent's team - based on images and videos, 

     % Another problem which needs to be solved is hardware based. To decide how to behave, real-time computations are necessary. Therefore t

    % Autonomous driving is the new big thing of the car industry. But not only companies in this business invest a huge amount of money, also the Computer and Software industry is seeking for the first hit on the market. Not long ago Mercedes tested the first lorry driving through the german highway without the help of men. As this topic becomes more impo

    % In the twenty-first century technical possibilities seem to be endless. Mobile phones act as small computers, the internet can answer almost every question and cars drive autonomous. But still, this development is an ongoing process. All these things have one thing in common: the science behind it.\\
    % Consider the mobile phone example. It needed a couple of engineers and physicists to develop processors smaller than a finger tipp. The suggestions you get using search engines like Google are based on algorithms which - mostly - where developed by computer scientists and mathematicians. Last ones also proof exactness and convergences of the algorithms to state that they are tractable.\\
    % These two examples also hold for autonomous driving. A lot of engineering needs to be done to let a car drive without a driver. One field not mentioned yet is 
    
% chapter introduction (end)