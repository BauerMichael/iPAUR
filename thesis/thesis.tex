\documentclass{scrreprt}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{dsfont}
% \usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{xcolor}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}

\pgfplotsset{compat=1.12}
\pgfplotsset{soldot/.style={color=blue,only marks,mark=*}}
\pgfplotsset{holdot/.style={color=blue,fill=white,only marks,mark=*}}

\input{settings.tex}

\begin{document}

\input{titlepage.tex}
\input{abstract.tex}
\input{acknowledgements.tex}
\input{00_introduction.tex}

\chapter{Basic Concepts} % (fold)
\label{cha:basic_concepts}

    In the first chapter we give an introduction to the most important concepts we meet in this thesis. We start by defining images in a mathematical manner, then introduce basic concepts of convex analysis and the total variation.

    \input{01_images}
    \input{02_convexity}
    \input{03_total_variation}

% chapter basic_concepts (end)

\chapter{A Primal-Dual Algorithm for (Convex) Saddle-Point Problems} % (fold)
\label{cha:a_first_order_primal_dual_algorithm_for_convex_saddle_point_problems}
    
    \input{04_the_general_saddle_point_problem}
    \input{05_a_first_order_primal_dual_algorithm}
    \input{06_discrete_setting}
    \input{07_the_rof_model}
    \input{08_the_tvl1_model}
    \input{09_the_mumford_shah_functional}

    In this chapter we provided a framework to solve variational methods, as well as a possibility to minimize the Mumford-Shah Functional. Applying this framework to real images and compare the results is then part of chapter \ref{cha:applications_to_imaging}. But first, we want to show a second way to minimize the Mumford-Shah Functional, in which we have a proof of optimiality for the solution.

% chapter a_first_order_primal_dual_algorithm_for_convex_saddle_point_problems (end)

\chapter{A Primal-Dual Algorithm for Minimizing the Mumford-Shah Functional} % (fold)
\label{cha:a_first_order_primal_dual_algorithm_for_minimizing_the_mumford_shah_functional}

    As we revisit the Mumford-Shah Functional, we also rewrite Definition \ref{def:the_mumford_shah_functional} to be consistend with \cite{Pock-et-al-iccv09}, which is the publication we mainly follow in this chapter. If other results are taken into account, we make this clear.

    \input{10_ms_convex_relaxation}
    \input{11_discrete_setting}
    \input{12_the_mumford_shah_functional}
    \input{13_projection_onto_convex_sets}
    \input{14_another_approach_using_lagrange_multiplier}

% chapter a_first_order_primal_dual_algorithm_for_minimizing_the_mumford_shah_functional (end)

\chapter{Applications to Imaging} % (fold)
\label{cha:applications_to_imaging}

    In this chapter we present applications to imaging for the presented models. We will compare run-time of different models by looking at the pure image approximation. We will see that the best method to solve the convex relaxed Mumford-Shah model is the algorithm based on Lagrange multipliers. Further, we consider image cartooning by using the real-time minimizer for the Mumford-Shah model, the ROF model and the TVL1 model. The last two models together with the convex relaxed Mumford-Shah model are then taken into account, when we show the denoising case. Last but not least we will see the most impressive application: image inpainting. The ROF model is able to reconstruct an images, which has a data loss of 70 percent.

    \section{Linearized Storage of Images} % (fold)
    \label{sec:linearized_storage_of_images}
        
        Images in a discrete two-dimensional grid of the size $N \times M$ can be viewed like matrices. The first element, which can be accessed is the one at the top left corner. Then they are stored row-wise from the left to the right side. The last element of this matrix is then at the bottom right. As mentioned in chapter \ref{cha:basic_concepts}, we do not store two-dimensional images in matrix form, but use a linearized version. In a programming language like C++, an image in matrix form would be accessed with
            $$
                image[i][j] = value;,
            $$
        where $i$ resembles the i-th row of the pixel grid and $j$ the j-th element in the corresponding row. It also admits, that we allocate a pointer to a pointer array and to access one element we need to look up two points in memory. This would slow down the code and for that we store all data only in one vector. We allocate a vector $v$ of the size $N \cdot M$. We attach each line of the image matrix to this array. Then we can access elements by
            $$
                v[j + i \cdot M] = value;.
            $$
        This method works well for grayscaled images, where we approximate an input images $g: \mathcal{G} \longrightarrow C = [0, 1]$ by a function $u: \mathcal{G} \longrightarrow C = [0, 1]$. So we map from the discret pixel grid $\mathcal{G}$ (see also notation \ref{eq:pixel_grid}) into the range from $0$ to $1$. Instead of using $[0, 1]$, we could also set $C = \{ 0, ..., 255 \}$, where we assume our image and approximation to have a 8-bit depth (see remark \ref{rem:continuous_vs_discrete}). Additionally note, that the vector $\bar{u}$ is stored in the same fashion, in the propose primal-dual algorithm.

        In the case of color images, in our case only RGB (red-green-blue) images, we can extend this kind of linearized storage easily. For that, we assume that each single color channel has its own pixel grid. We have for $k = 1, 2, 3$
            \begin{equation}
                \mathcal{G}_{k} = \bigg\{ (i, j) : i = 1, ..., N \,\, \textnormal{and} \,\, j = 1, ..., M \bigg\}.
                \label{eq:color_pixel_grid}
            \end{equation}
        Overall, we attach each row of each grid separately to the vector $img$ consecutively. Then, we have that $img \in \mathbb{R}^{N \cdot M \cdot 3}$ and we access one element by
            $$
                img[j + i \cdot M + k \cdot M \cdot N] = value;.
            $$

        Furthermore, we need to consider the variables $p$ in the primal-dual algorithm. These are also stored as vectors, but with the extension, that for a fixed point $(i, j)$ (or $(i, j, k)$ in the case of color images) we have $p_{i,j(,k)} \in \mathbb{R}^{2}$. We have two possibilities how to handle the storage of these variables. The easiest way, which we choose to use, would be to allocate two vectors of the size $N \cdot M \cdot 3$ and call them for instance $p_{x}$ and $p_{y}$, respectively.
        Another possibility is to do the same as before: attach the values of $p_{y}$ to these of $p_{x}$ and observe one large array, which then can be accessed by
            $$
                p[j + i \cdot M + k \cdot M \cdot N + l \cdot M \cdot N \cdot 3] = value;,
            $$
        with $l = 1, 2$. For us it seemed to be necessary to use the first method, to obtain a readable and maintainable code. At last, we want to mention, that in our case using C++ and the CUDA framework, the access of elements starts with $0$ and the last element of a n-dimensional array is then accessed at the location $n-1$.

    % section linearized_storage_of_images (end)

    \input{15_image_approximation}
    \input{16_image_cartooning}
    % \input{17_image_denoising}
    % \input{18_image_inpainting}
    % \input{19_mumford_shah_comparison}

% chapter applications_to_imaging (end)

\chapter{Conclusion} % (fold)
\label{cha:conclusion}

    % \input{19_conclusion}

% chapter conclusion (end)

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}