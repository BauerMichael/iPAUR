\section{A First-Order Primal-Dual Algorithm} % (fold)
\label{sec:a_firs_order_primal_dual_algorithm}

    Our goal is to solve the saddle-point problem \ref{eq:the_saddle_point_problem}. Therefore, one finds in a series of paper the celebrated (fast) first-order Primal-Dual Algorithm. According to \cite{Chambolle-et-al-10} the idea of this method for solving saddle-point problems goes back to Arrow and Hurwicz, see also \cite{Arrow-Hurwicz}. For that reason these primal-dual approaches are sometimes also called Arrow-Hurwicz methods. The first time this algorithm was stated in a framework was probably in \cite{Appleton-Talbot}. The general idea of the proposed algorithm is to do a gradient descent in $u$, since this is the variable of the minimzation problem. And do, simultaneously, a gradient ascent in $p$, because this is the variable of the maximization problem. Choosing time-steps $\sigma, \tau > 0$ one gets
        \begin{eqnarray}
            p^{n+1} = (\textnormal{Id} + \sigma\,\partial\,F^{\ast})^{-1}(p^{n} + \sigma\,Ku^{n}) \notag \\
            u^{n+1} = (\textnormal{Id} + \tau\,\partial\,G)^{-1}(u^{n} - \tau\,K^{\ast}p^{n+1}). \notag
        \end{eqnarray}
    The algorithm is proposed in a paper of Zhu and Chan in \cite{Zhu-Chan}. Unfortunatelly, there is no proof of convergence for this scheme. This makes the approach poor. But in 2009 Pock, Cremers, Bischof and Chambolle proposed a paper, that we also consider in Chapter \ref{cha:a_first_order_primal_dual_algorithm_for_minimizing_the_mumford_shah_functional}, whose contribution was a provable extension of the above scheme. The idea here was to add an additional extrapolation step to the algorithm, as seen in line three of the primal-dual algorithm. In \cite{Chambolle10afirst-order} Pock and Chambolle generalized this algorithm. They also proposed some variations of the algorithm itself. Depending on the properties of the corresponding functions $F^{\ast}$ and $G$ one can derive a better convergence rate. We will not provide details and just make use of two algorithms. In Chapter \ref{cha:applications_to_imaging}, we apply the two versions to different problems in imaging. Further, we will not provide a proof of convergence for these algorithms. For details we refer to \cite{Chambolle10afirst-order} and \cite{Pock-et-al-iccv09}. Then the general fast primal-dual algorithm is as follows

    \begin{algorithm}[First-Order Primal-Dual Algorithm]
    \label{alg:fast_primal_dual_algorithm}
        Choose $(u^{0}, p^{0}) \in X \times Y$ and let $\bar{u}^{0} = u^{0}$. Further let $\tau, \sigma > 0$ with $\sigma\tau L^{2} \le 1$ and $\theta \in [0, 1]$. Then, we let for each $n \ge 0$
            \begin{equation}
                \left\{ 
                    \begin{array}{l l}
                        p^{n+1} = (\textnormal{Id} + \sigma\,\partial\,F^{\ast})^{-1}(p^{n} + \sigma\,K\bar{u}^{n}) \\
                        u^{n+1} = (\textnormal{Id} + \tau\,\partial\,G)^{-1}(u^{n} - \tau\,K^{\ast}p^{n+1}) \\
                        \bar{u}^{n+1} = u^{n+1} + \theta (u^{n+1} - u^{n}).
                    \end{array}
                \right.
            \label{eq:fast_primal_dual_algorithm}
            \end{equation}
    \end{algorithm}

    Additionally, let us introduce the primal-dual gap, which is strongly related to the weak and strong duality theorems (found for instance in \cite{Geiger-Kanzow}. The primal-dual gap will be a part of the convergences theorem \ref{the:primal_dual_convergence}.
    \begin{definition}[Primal-Dual Gap] % (fold)
    \label{def:primal_dual_gap}

        Let $u \in X$, $p \in Y$ be the variables of the optimization problem in Equation \ref{eq:the_saddle_point_problem}. Then we define the primal-dual gap of this problem by
            \begin{equation}
                \mathcal{G}(u, p) = \max_{\tilde{p} \in Y} \langle \tilde{p}, Ku \rangle - F^{\ast}(\tilde{p}) + G(u) - \min_{\tilde{u} \in X} \langle p, K\tilde{u} \rangle - F^{\ast}(p) + G(\tilde{u}),
                % \mathcal{G}(u, p) = F(Ku) + G(u) + G^{\ast}(-K^{\ast}p) - F^{\ast}(p),
                \label{eq:primal_dual_gap}
            \end{equation}
        which has the property that $\mathcal{G}(u, p) \ge 0$ for all $u, p$ and equality only holds if and only if $(u, p)$ is a saddle-point. If $\hat{p}$ is a solution of the maximization problem and $\hat{u}$ a solution of the minimization problem the following inequality holds:
            \begin{equation}
                \mathcal{G}(u, p) \ge \langle \hat{p}, Ku \rangle - F^{\ast}(\hat{p}) + G(u) - \langle p, K\hat{u} \rangle - F^{\ast}(p) + G(\hat{u}) \ge 0
                \label{eq:primal_dual_gap}
            \end{equation}
    \end{definition}
    % definition primal_dual_gap (end)

    For this particular algorithm one can find a convergence theorem in \cite{Chambolle10afirst-order}, which we provide without a proof.

    \begin{theorem} % (fold)
    \label{the:primal_dual_convergence}
        Let $L = ||K||$ and assume Equation \ref{eq:the_saddle_point_problem} has a saddle-point $(\hat{u}, \hat{p})$. Choose $\theta = 1, \tau, \sigma, L^{2} < 1$, and let $(u^{n}, \bar{u}^{n}, p^{n})$ be defined by \ref{eq:fast_primal_dual_algorithm}. Then
            \begin{enumerate}
                \item For any n,
                    $$
                        \frac{||p^{n} - \hat{p}||^{2}}{2\sigma} + \frac{||u^{n} - \hat{u}||^{2}}{2\tau} \le C \bigg( \frac{||p^{0} - \hat{p}||^{2}}{2\sigma} + \frac{||u^{0} - \hat{u}||^{2}}{2\tau} \bigg),
                    $$
                where the constant $C \le (1 - \tau\sigma L^{2})^{-1}$.
                \item If we let $u^{N} = \bigg( \frac{\sum\limits_{n=1}^{N} u^{n}}{N} \bigg)$ and $p^{N} = \bigg( \frac{\sum\limits_{n=1}^{N} p^{n}}{N} \bigg)$, for any bounded $B_{1} \times B_{2} \subset X \times Y$ the restricted primal-dual gap has the following bound:
                    $$
                        \mathcal{G}_{B_{1} \times B_{2}}(u^{N}, p^{N}) \le \frac{D(B_{1}, B_{2})}{N},
                    $$
                where
                    $$
                        D(B_{1}, B_{2}) = \sup_{(u, p) \in B_{1} \times B_{2}} \frac{||u - u^{0}||^{2}}{2\tau} + \frac{||p - p^{0}||^{2}}{2\sigma}.
                    $$
                Moreover, the weak cluster points of $(u^{N}, p^{N})$ are saddle-points of \ref{eq:the_saddle_point_problem}.
                \item If the dimension of the spaces $X$ and $Y$ is finite, then there exists a saddle-point $(u^{\ast}, p^{\ast})$, such that $u^{n} \longrightarrow u^{\ast}$ and $p^{n} \longrightarrow p^{\ast}$.
            \end{enumerate}
    \end{theorem}

    \begin{remark}
        What this theorem states is, that one needs to choose $\tau, \sigma$ carefully by initializing the algorithm. As long as the inequality $\tau\sigma L^{2} < 1$ holds, convergence is guaranteed. The two parameters $\tau, \sigma$ are also sometimes called time-steps. The better the choice for these beforhand, the faster the algorithm converges. Of course, it is not the best way having an algorithm which is dependent on manual choices, but on the other hand two parameter are highly controllable and one gets fast convergence. Other methods are almost as fast as the primal-dual algorithm, but depend on a couple of parameters, or they are independent of parameter choices and have a slow convergence rate.
    \end{remark}

    As mentioned, sometimes this method for solving saddle-point problems is also called Arrow-Hurwicz methods. To derive their original algorithm one would need to choose $\theta = 0$. We will not consider this case in our computations. The convergence rate for the primal-dual algorithm is $\mathcal{O}(\frac{1}{N})$. In the case that one function, $F^{\ast}$ or $G$, is uniformly convex one gets:

    \begin{algorithm}[$F^{\ast}$ or $G$ are uniformly convex]
    \label{alg:f_star_or_g_uniformly_convex}
        Choose $(u^{0}, p^{0}) \in X \times Y$ and let $\bar{u}^{0} = u^{0}$. Further let $\tau_{0}, \sigma_{0}, \gamma > 0$ with $\sigma\tau L^{2} \le 1$. Then, we let for each $n \ge 0$
            \begin{equation}
                \left\{ 
                    \begin{array}{l l}
                        p^{n+1} = (\textnormal{Id} + \sigma_{n}\,\partial\,F^{\ast})^{-1}(p^{n} + \sigma_{n}\,K\bar{u}^{n}) \\
                        u^{n+1} = (\textnormal{Id} + \tau_{n}\,\partial\,G)^{-1}(u^{n} - \tau_{n}\,K^{\ast}p^{n+1}) \\
                        \theta_{n} = \frac{1}{\sqrt{1 + 2\gamma\tau_{n}}}, \, \tau_{n+1} = \theta_{n}\tau_{n}, \, \sigma_{n+1} = \frac{\sigma_{n}}{\theta_{n}}
                        \bar{u}^{n+1} = u^{n+1} + \theta_{n} (u^{n+1} - u^{n}).
                    \end{array}
                \right.
            \label{eq:f_star_or_g_uniformly_convex}
            \end{equation}
    \end{algorithm}

    This leads to a convergence rate of $\mathcal{O}(\frac{1}{N})$. Once parallelized, these algorithms will run in real-time. Fortunatelly, both are highly parallelizable on a GPU using the CUDA framework. In Chapter \ref{cha:applications_to_imaging} we provide in-depth details of this.

    The computations in each line are straightforward. One has to compute sums of vectors and scaled vectors. Since, we will set $K = \nabla$ and for that its conjugate is $-\textnormal{div}$ or equivalently $- \nabla^{T}$, calculating the operator is also easy. The main work needs to be done to find the proximity operators for $G$ and $F^{\ast}$, respectively. They vary within different models. For that, we need to find these operators for each and every model.

    % \begin{algorithm}[$F^{\ast}$ and $G$ are uniformly convex]
    % \label{alg:f_star_and_g_uniformly_convex}
    %     Choose $(u^{0}, p^{0}) \in X \times Y$ and let $\bar{u}^{0} = u^{0}$. Further let $\mu \le \frac{2\sqrt{\gamma\delta}}{L}, \tau = \frac{\mu}{2\gamma}, \sigma = \frac{\mu}{2\delta}$, $\theta \in [\frac{1}{1 + \mu}, 1]$. Then, we let for each $n \ge 0$
    %         \begin{equation}
    %             \left\{ 
    %                 \begin{array}{l l}
    %                     p^{n+1} = (\textnormal{Id} + \sigma\,\partial\,F^{\ast})^{-1}(p^{n} + \sigma\,K\bar{u}^{n}) \\
    %                     u^{n+1} = (\textnormal{Id} + \tau\,\partial\,G)^{-1}(u^{n} - \tau\,K^{\ast}p^{n+1}) \\
    %                     \bar{u}^{n+1} = u^{n+1} + \theta (u^{n+1} - u^{n}).
    %                 \end{array}
    %             \right.
    %         \label{eq:f_star_and_g_uniformly_convex}
    %         \end{equation}
    % \end{algorithm}

% section a_firs_order_primal_dual_algorithm (end)