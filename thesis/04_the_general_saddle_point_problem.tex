\section{The General Saddle-Point Problem} % (fold)
\label{sec:the_general_saddle_point_problem}
    
    In this first section we will present the general class of problem we consider in this work. Therefore we define the the map $K: X \longrightarrow Y$ as a continuous linear operator with induced norm
        \begin{equation}
            ||K|| = \max \big\{ ||Kx||_{Y} : x \in X \,\, \textnormal{with} \,\, ||x||_{X} \le 1 \big\}.
            \label{eq:operator_norm}
        \end{equation}

    Furthermore, we set the map $K^{\ast}: Y \longrightarrow X$ as the adjoint operator of $K$.\\
    Now, let $K$ be a linear operator, $u \in X$, $p \in Y$ and define $G: X \longrightarrow \mathbb{R}_{+}$ and $F^{\ast}: Y \longrightarrow \mathbb{R}_{+}$ where $G, F^{\ast} \in \Gamma_{0}$ and $F^{\ast}$ being a Legendre-Fenchel conjugate of a convex, l.s.c. function $F$. We are trying to find the saddle points of the following problem:
        \begin{equation}
            \min_{u \in X} \max_{p \in Y} \langle Ku, p \rangle + G(u) - F^{\ast}(p).
            \label{eq:the_saddle_point_problem}
        \end{equation}
    We call this problem also the primal-dual-problem, where $u$ is the primal and $p$ the dual variable. With the results of Section \ref{sec:convex_optimization_and_convex_analysis} we will define the corresponding primal-problem to this formulation by
        \begin{equation}
            \min_{u \in X} F(Ku) + G(u),
            \label{eq:primal_problem}
        \end{equation}
    and the corresponding dual-problem by
        \begin{equation}
            \max_{p \in Y} -(G^{\ast}(-K^{\ast}p) + F^{\ast}(p)).
            \label{eq:dual_problem}
        \end{equation}
    These three different classes of problems are equivalent since the Legendre-Fenchel conjugate assures that
        \begin{eqnarray}
            \min_{u \in X} \underbrace{F(Ku)}_{=\, \sup_{p \in Y} \langle p, Ku \rangle - F^{\ast}(p)} + G(u) &=& \min_{u \in X} \max_{p \in Y} \langle p, Ku \rangle - F^{\ast}(p) + G(u) \notag \\
            &=& \max_{p \in Y} \min_{u \in X} \langle K^{\ast}p, u \rangle + G(x) - F(p) \notag \\
            &=& \max_{p \in Y} \underbrace{\max_{u \in X} -\big( \langle K^{\ast}p, u \rangle + G(x) \big)}_{=\, -(G^{\ast}(-K^{\ast})p)} - F(p) \notag \\
            &=& \max_{p \in Y} -(G^{\ast}(-K^{\ast}p) + F^{\ast}(p))
            \label{eq:swap_min_to_max}
        \end{eqnarray}
    Note that we can swap $\min$ and $\max$ and exchange $\sup$ by $\max$ since we are acting on finite, normed vector spaces. For this reason it also equivalent to seek for the maximimum of a function $u$ or instead seek for the minimum of the function $-u$.
    % Additionally, let us introduce the primal-dual gap:

    % \begin{definition}[Primal-Dual Gap] % (fold)
    % \label{def:primal_dual_gap}

    %     Let $u \in X$, $p \in Y$ be the variables of the optimization problem in Equation \ref{eq:the_saddle_point_problem}. Then we define the primal-dual gap of this problem by
    %         \begin{equation}
    %             \mathcal{G}(u, p) = \max_{\tilde{p} \in Y} \langle \tilde{p}, Ku \rangle - F^{\ast}(\tilde{p}) + G(u) - \min_{\tilde{u} \in X} \langle p, K\tilde{u} \rangle - F^{\ast}(p) + G(\tilde{u}),
    %             % \mathcal{G}(u, p) = F(Ku) + G(u) + G^{\ast}(-K^{\ast}p) - F^{\ast}(p),
    %             \label{eq:primal_dual_gap}
    %         \end{equation}
    %     which has the property that $\mathcal{G}(u, p) \ge 0$ for all $u, p$ and equality only holds if and only if $(u, p)$ is a saddle-point. If $\hat{p}$ is a solution of the maximization problem and $\hat{u}$ a solution of the minimization problem the following inequality holds:
    %         \begin{equation}
    %             \mathcal{G}(u, p) \ge \langle \hat{p}, Ku \rangle - F^{\ast}(\hat{p}) + G(u) - \langle p, K\hat{u} \rangle - F^{\ast}(p) + G(\hat{u}) \ge 0
    %             \label{eq:primal_dual_gap}
    %         \end{equation}
    % \end{definition}
    % % definition primal_dual_gap (end)

    Assuming that problem \ref{eq:the_saddle_point_problem} we are considering has at least one solution, which we denote by $(\hat{u}, \hat{p}) \in X \times Y$, then $\hat{u}$ as the solution to the primal-problem satisfies
        \begin{equation}
            K\hat{u} \in \partial F^{\ast}(\hat{p}),
            \label{eq:kx_in_subgradient}
        \end{equation}
    and $\hat{p}$ as the solution of the dual-problem satisfies
        \begin{equation}
            -(K^{\ast}\hat{p}) \in \partial G(\hat{u}).
            \label{eq:k_star_y_in_subgradient}
        \end{equation}
        
    \begin{proof}
        If we define a function $L(u, p) := \langle K^{\ast}p, u \rangle - F^{\ast}(p) + G(u)$ in the sense of Equations \ref{eq:swap_min_to_max}, we have
            $$
                \max_{p \in Y} \min_{u \in X} L(u, p)
            $$
        Now, fixing the maximal value over all $p$ and denote this by $\hat{p}$ we get the following minimzation problem
            $$
                \min_{u \in X} L(u, \hat{p}).
            $$
        According to Proposition \ref{prop:zero_element_of_subgradient} this is equivalent to $0 \in \partial\,L(\hat{u}, \hat{p})$ if $\hat{u}$ is the minimizer of the optimization problem. From this we suddenly observe Equation \ref{eq:k_star_y_in_subgradient}, since
            $$
                0 \in \partial \, (\langle K^{\ast}\hat{p}, \hat{u} \rangle - F^{\ast}(\hat{p}) + G(\hat{u})) \Longleftrightarrow 0 \in K^{\ast}\hat{p} + \partial \, G(\hat{u}) \Longleftrightarrow -K^{\ast}\hat{p} \in \partial \, G(\hat{u}).
            $$
        Doing the same by fixing the minimum of $u$, denoted by $\hat{u}$ we observe
            $$
                \max_{p \in Y} L(\hat{u}, p) \Longleftrightarrow \min_{p \in Y} -L(\hat{u}, p).
            $$
        Again, using Proposition \ref{prop:zero_element_of_subgradient} we get
            $$
                0 \in \partial \, -(\langle \hat{p}, K\hat{u} \rangle - F^{\ast}(\hat{p}) + G(\hat{u})) \Longleftrightarrow 0 \in -K\hat{u} + \partial \, F^{\ast}(\hat{p}) \Longleftrightarrow K\hat{u} \in F^{\ast}(\hat{p}),
            $$
        which proves Equation \ref{eq:kx_in_subgradient}.\qed
    \end{proof}

% section the_general_saddle_point_problem (end)